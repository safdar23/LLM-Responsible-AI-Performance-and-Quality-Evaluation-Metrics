# LLM-Responsible-AI-Performance-and-Quality-Evaluation-Metrics

Large language models metrics can be measured using the below measurement methods for question-answering and conversation generative AI applications:

Traditional Machine Learning Metrics

AI-Assisted Metrics: These metrics evaluate AI-generated output, even when there is no predefined ground truth. These metrics are further  segregated into two types

 ## Responsible AI metrics
 
 ## Performance and Quality Metrics
 

# Traditional Machine Learning Metrics

These metrics, such as the F1 score, measure precision and recall by comparing AI-generated responses to expected answers. The F1-score measures the number of shared words between the generated response and the ground truth and calculated precision and recall. Precision is the ratio of the number of shared words to the total number of words in the generated response, and recall is the ratio of the number of shared words to the total number of words in the ground truth.

# AI Assisted: Performance and Quality metrics

These metrics assess the quality and coherence of the generated content, including coherence, fluency, groundedness, relevance, retrieval score, and similarity.


# AI Assisted: Responsible AI Metrics
Azure AI identifies content and security risks, including contents related to hateful, unfair, sexual, violent, self-harm, and jailbreak defects. Azure responsible AI scores these contents severity across (very Low (0–1), Low (2–3), Medium (4–5) and High (6–7). High (6–7) is the extremely rated content. For detailed information, please visit Evaluation and monitoring metrics for generative AI - Azure AI Studio | Microsoft Learn
